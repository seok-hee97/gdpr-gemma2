# GDPR-Gemma-2-2B


# Project Title
- GDPR  text-classification / text-generation

[thinking]
- DLP :  text-classification 이 좀 더 적합.    
  keyword : DLP(data loss protection), GDPR     
- DLP hard to apply general?? -> Apply at GDPR(it's privacy rule).  
- make GDPR gemma2 model  2-way   
  (text-classification / text- generation).     

## Project Description


## (Optional) Reference URLs (e.g. github repo, arxiv paper)

## Your Name
name : 장석희

## Product to use for your project
(Gemma2 2B/ 9B/ 27B)

Choose : Gemma2 2B

## Output format (Select all)
(model on Kaggle/Hugging Face | app or code | tutorial(blog or Youtube))

Choose : model on Kaggle/Hugging Face



#### project-reference
- QLoRa Fine-tuning
  - [Fine-tuning Gemma with QLoRa](https://medium.com/google-developer-experts/fine-tuning-gemma-with-qlora-407e56c36026)
  - [Fine-Tune Gemma Using QLoRA](https://medium.com/@samvardhan777/fine-tune-gemma-using-qlora-%EF%B8%8F-6b2f2e76dc55)
- Code Reference
  - [(github)UKPLab/sentance-transformers](https://github.com/UKPLab/sentence-transformers)
  - [(cookbook)Aligning_DPO_Gemma_2b_it.ipynb](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Aligning_DPO_Gemma_2b_it.ipynb)
  - 
- Related LLM project
  - [(hugging face)sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2](https://huggingface.co/sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2)
(summary :해당 질문이 GDPR 잘 준수했는지 확인해주는 LLM 모델)
- Concept
  - [Data Loss Prevention, an EU/GDPR perspective](https://grcoutlook.com/data-loss-prevention-an-eu-gdpr-perspective/)
  - [How to Cato Uses Large Language Models to Improve Data Loss Prvention](https://www.catonetworks.com/blog/how-cato-uses-large-language-models-to-improve-data-loss-prevention/)


#### Dataset
- [sims2k/GDPR_QA_instruct_dataset](https://huggingface.co/datasets/sims2k/GDPR_QA_instruct_dataset)
- [sims2k/GDPR_QA_instruct_eval_dataset](https://huggingface.co/datasets/sims2k/GDPR_QA_instruct_eval_dataset)
- [(github)gdpr-dataset](https://github.com/tamjidrahat/gdpr-dataset)
- [Is Your Policy Compliant?: A Deep Learning-based Empirical Study of Privacy Policies' Compliance with GDPR](https://dl.acm.org/doi/10.1145/3559613.3563195)


## IDEA
- [Insights into Data Quality and Evaluation in Gemma 2 LLM](https://kili-technology.com/large-language-models-llms/insights-into-data-quality-and-evaluation-in-gemma-2-llm)
- [AI in Cybersecurity: Exploring the Top 6 Use Cases](https://www.techmagic.co/blog/ai-in-cybersecurity/)
- [LLM meets Malware: Starting the Era of Autonomous Threat](https://medium.com/@b42labs/llm-meets-malware-starting-the-era-of-autonomous-threat-e8c5827ccc85)


## Gemma Model
- [(구글코라이 블로그)구글의 최첨단 오픈 모델 '젬마(Gemma)'를 공개합니다](https://blog.google/intl/ko-kr/products/explore-get-answers/-gemma-open-models-kr/)
- [Gemma 2 model card](https://ai.google.dev/gemma/docs/model_card_2#model_information)
- [Encoder Only 와 Decoder Only 언어모델에 대한 고찰](https://medium.com/@hugmanskj/encoder-only-%EC%99%80-decoder-only-%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%9C-%EA%B3%A0%EC%B0%B0-9852213dbb72)


## reference
- [(youtube)Fine-tuning LLMs | w/ Example Code](https://www.youtube.com/watch?v=eC6Hd1hFvos)
- [Low-Rank Adapter (LoRA) Explained](https://medium.com/@shelikohan/low-rank-adapter-lora-explained-0d3677395639)
- [(medium)Getting Started with Google's Gemma LLM using HuggingFace Libaries](https://medium.com/@coldstart_coder/getting-started-with-googles-gemma-llm-using-huggingface-libraries-a0d826c552ae)
- [(DEVOCEAN) Gemma 한국어 요약 모델 파인튜닝 빠르게 해보기](https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog&ref=blog.update.sh)
- [(DEVOCEAN)오픈소스 LLM에 새로운 표준을 제시할 구글 Gemma](https://devocean.sk.com/blog/techBoardDetail.do?ID=165709)
- [(blog)Gemma: Open Models Based on GeminiResearch and Technology 논문 리뷰](https://wiz-tech.tistory.com/entry/Gemma-Open-Models-Based-on-GeminiResearch-and-Technology-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
- [Sherlock Holmes Q&A with Gemma fine tuning](https://www.kaggle.com/code/lucamassaron/sherlock-holmes-q-a-with-gemma-fine-tuning/notebook)

### 참고 리소스(Instructions)
- [Gemma](https://ai.google.dev/gemma/) 
- [Announcement](https://blog.google/technology/developers/google-gemma-2/)
- [Docs](https://ai.google.dev/gemma/docs)
- [Blog posts](https://developers.googleblog.com/en/search/?query=gemma&product_categories=Gemma) 
- [Cookbook](https://github.com/google-gemini/gemma-cookbook)

### 커뮤니티

- [Build with Google AI forum](https://discuss.ai.google.dev/)
- [Discord 채널](https://discord.com/channels/1009525727504384150/1209857547390025768)





[final]
# Gemma-Sprint

# Project Title
- GDPR  text-classification / text-generation

## Project Description


## (Optional) Reference URLs (e.g. github repo, arxiv paper)

- [sims2k/GDPR_QA_instruct_dataset](https://huggingface.co/datasets/sims2k/GDPR_QA_instruct_dataset?sql=--+The+SQL+console+is+powered+by+DuckDB+WASM+and+runs+entirely+in+the+browser.%0A--+Get+started+by+typing+a+query+or+selecting+a+view+from+the+options+below.%0ASELECT+*+FROM+train+LIMIT+10%3B)

- [sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2](https://huggingface.co/sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2)
## Your Name
name : 장석희

## Product to use for your project
(Gemma2 2B/ 9B/ 27B)

Choose : Gemma2 2B

## Output format (Select all)
(model on Kaggle/Hugging Face | app or code | tutorial(blog or Youtube))

Choose : model on Kaggle/Hugging Face



#### project-reference
- QLoRa Fine-tuning
  - [Fine-tuning Gemma with QLoRa](https://medium.com/google-developer-experts/fine-tuning-gemma-with-qlora-407e56c36026)
  - [Fine-Tune Gemma Using QLoRA](https://medium.com/@samvardhan777/fine-tune-gemma-using-qlora-%EF%B8%8F-6b2f2e76dc55)
- Code Reference
  - [(github)UKPLab/sentance-transformers](https://github.com/UKPLab/sentence-transformers)
  - [(cookbook)Aligning_DPO_Gemma_2b_it.ipynb](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Aligning_DPO_Gemma_2b_it.ipynb)
  
- Related LLM project
  - [(hugging face)sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2](https://huggingface.co/sims2k/Saul-Instruct-v1-gdpr-finetuned-v5.2)
(summary :해당 질문이 GDPR 잘 준수했는지 확인해주는 LLM 모델)
- Concept
  - [Data Loss Prevention, an EU/GDPR perspective](https://grcoutlook.com/data-loss-prevention-an-eu-gdpr-perspective/)
  - [How to Cato Uses Large Language Models to Improve Data Loss Prvention](https://www.catonetworks.com/blog/how-cato-uses-large-language-models-to-improve-data-loss-prevention/)


#### Dataset
- [GDPR_QA_instruct_dataset](https://huggingface.co/datasets/sims2k/GDPR_QA_instruct_dataset)
- 
- [(github)gdpr-dataset](https://github.com/tamjidrahat/gdpr-dataset)
- [Is Your Policy Compliant?: A Deep Learning-based Empirical Study of Privacy Policies' Compliance with GDPR](https://dl.acm.org/doi/10.1145/3559613.3563195)


## IDEA
- [Insights into Data Quality and Evaluation in Gemma 2 LLM](https://kili-technology.com/large-language-models-llms/insights-into-data-quality-and-evaluation-in-gemma-2-llm)
- [AI in Cybersecurity: Exploring the Top 6 Use Cases](https://www.techmagic.co/blog/ai-in-cybersecurity/)
- [LLM meets Malware: Starting the Era of Autonomous Threat](https://medium.com/@b42labs/llm-meets-malware-starting-the-era-of-autonomous-threat-e8c5827ccc85)


## Gemma Model
- [(구글코라이 블로그)구글의 최첨단 오픈 모델 '젬마(Gemma)'를 공개합니다](https://blog.google/intl/ko-kr/products/explore-get-answers/-gemma-open-models-kr/)
- [Gemma 2 model card](https://ai.google.dev/gemma/docs/model_card_2#model_information)
- [Encoder Only 와 Decoder Only 언어모델에 대한 고찰](https://medium.com/@hugmanskj/encoder-only-%EC%99%80-decoder-only-%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%9C-%EA%B3%A0%EC%B0%B0-9852213dbb72)


## reference
- [(youtube)Fine-tuning LLMs | w/ Example Code](https://www.youtube.com/watch?v=eC6Hd1hFvos)
- [Low-Rank Adapter (LoRA) Explained](https://medium.com/@shelikohan/low-rank-adapter-lora-explained-0d3677395639)
- [(medium)Getting Started with Google's Gemma LLM using HuggingFace Libaries](https://medium.com/@coldstart_coder/getting-started-with-googles-gemma-llm-using-huggingface-libraries-a0d826c552ae)
- [(DEVOCEAN) Gemma 한국어 요약 모델 파인튜닝 빠르게 해보기](https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog&ref=blog.update.sh)
- [(DEVOCEAN)오픈소스 LLM에 새로운 표준을 제시할 구글 Gemma](https://devocean.sk.com/blog/techBoardDetail.do?ID=165709)
- [(blog)Gemma: Open Models Based on GeminiResearch and Technology 논문 리뷰](https://wiz-tech.tistory.com/entry/Gemma-Open-Models-Based-on-GeminiResearch-and-Technology-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
- [Sherlock Holmes Q&A with Gemma fine tuning](https://www.kaggle.com/code/lucamassaron/sherlock-holmes-q-a-with-gemma-fine-tuning/notebook)

### 참고 리소스(Instructions)
- [Gemma](https://ai.google.dev/gemma/) 
- [Announcement](https://blog.google/technology/developers/google-gemma-2/)
- [Docs](https://ai.google.dev/gemma/docs)
- [Blog posts](https://developers.googleblog.com/en/search/?query=gemma&product_categories=Gemma) 
- [Cookbook](https://github.com/google-gemini/gemma-cookbook)

### 커뮤니티

- [Build with Google AI forum](https://discuss.ai.google.dev/)
- [Discord 채널](https://discord.com/channels/1009525727504384150/1209857547390025768)



--------------------------------------------------------------------


## **Instruction**

### 공통사항
- Gemma2를 이용한 모델, 코드, 앱, 튜토리얼 중 하나의 프로젝트 결과물을 만듭니다.
- 개인~ 최대 3인의 팀으로으로 참여할 수 있습니다.
- 스스로 고안한 프로젝트 아이디어로 참가해야 합니다. 
- 프로젝트가 아이디어가 정해지면 프로젝트 시트의 A열~F열을 작성합니다. (F열은 2개 이상 선택할 수 있습니다) 
- Gemma 스터디/개발기 중간 공유 등을 호스팅하시는 것도 좋습니다.
- 프로젝트 및 프로젝트 시트는 '영어로' 작성해야 합니다. 
- 실수로 다른 참가자의 내용을 삭제하거나 수정한 경우 바로 관리자에게 DM으로 알려주세요. (작성은 아래쪽으로 순서대로 작성해주세요. 행 추가 x)
  
### 프로젝트 제출하기
#### Fine-tuned Model
- Kaggle Models 혹은 Hugging Face Models에 파인튜닝한 Gemma 모델을 배포하고 Kaggle Code (Notebook) / Hugging Face Model Card에 모델 설명, 모델 학습 과정, 코드, 성능/결과 등을 함께 작성해야 합니다. 
- 튜토리얼/데모 등을 추가로 제작하는 것도 권장합니다. 
#### App, Code, Tutorial
- 코드는 GitHub, Google Colab, Jupyter Notebook(읽기 링크 포함) 등 어떤 플랫폼/환경도 사용할 수 있습니다.
- 튜토리얼은 블로그 또는 YouTube로 제작해야 합니다. ('한국어' 가능)

### **결과물 제출하기 (Deadline : 10/4)!!**   
- 프로젝트가 완료되면 G나 H열을 작성하고, 이해를 돕는 추가 결과물들을 I열에 추가하세요. '활성화'된 소셜(블로그, X, 링크드인, 유튜브 등)에 #GemmaSprint 해시태그와 함께 결과물을 공유해주세요. (필수)
- 프로젝트 완료 & 소셜 공유 마감일은 10월 4일(금) 오전 10시입니다.
- 완주하여 completed에 체크 후 & 참여 대시보드의 results에 'O' 표기가 확인되면, 이 양식에서 완주 기념품을 선택해주세요. (팀으로 진행했어도 각자 접수)


### 완주 기념품   
- 기본 완주 기념품: Gemma 티셔츠+뱃지, Gemini 티셔츠, G로고 티셔츠 중 택1
- Model 배포 완주 기념품: Gemini 백팩, Gemini 재킷 중 택1
- 우수 프로젝트 기념품: 파타고니아 백팩 (한정 수량)

### 참고 리소스
튜토리얼, 예시, Docs는 아래 링크를 참고하세요
튜토리얼, 예시, Docs는 아래 링크를 참고하세요. 

- [Gemma](https://ai.google.dev/gemma/) 
- [Announcement](https://blog.google/technology/developers/google-gemma-2/)
- [Docs](https://ai.google.dev/gemma/docs)
- [Blog posts](https://developers.googleblog.com/en/search/?query=gemma&product_categories=Gemma) 
- [Cookbook](https://github.com/google-gemini/gemma-cookbook)

### 커뮤니티

- [Build with Google AI forum](https://discuss.ai.google.dev/)
- [Discord 채널](https://discord.com/channels/1009525727504384150/1209857547390025768)
  
-----------------------------------------------------------------






### (Plan B) Gemma2 Project

# Project Title
KorGemma-Academic: Enhancing Korean Academic Writing with Gemma 2B

## Project Description
This project aims to fine-tune the Gemma 2B model specifically for improving Korean academic writing. 
The model will focus on enhancing the clarity, coherence, and formal style of academic texts in Korean, 
assisting researchers, students, and academics in producing high-quality scholarly work.
## (Optional) Reference URLs (e.g. github repo, arxiv paper)

## Your Name
name : 장석희
## Product to use for your project
(Gemma2 2B/ 9B/ 27B)

Choose : Gemma2 2B

## Output format (Select all)
(model on Kaggle/Hugging Face | app or code | tutorial(blog or Youtube))

Choose : model on Kaggle/Hugging Face

## DataSet
- [Hugging Face Dataset](https://huggingface.co/datasets)
- AI Hub




--------------------------------------------